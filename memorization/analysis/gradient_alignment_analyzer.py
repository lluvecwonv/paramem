#!/usr/bin/env python3
"""
Gradient Alignment Analysis for Memorization Transfer Detection

Simple implementation based on Definition 3.3:
Align(P(x), x; Î¸) = 1/K * Î£ <g(x'_i; Î¸), g(x; Î¸)>

ALIGNMENT METRICS EXPLANATION:
1. Inner Product (ë‚´ì ): <g(x), g(x')>
   - ì›ì‹œ ê·¸ë¼ë””ì–¸íŠ¸ ë²¡í„° ê°„ì˜ ë‚´ì 
   - í¬ê¸°(magnitude)ì— ë¯¼ê°: í° ê·¸ë¼ë””ì–¸íŠ¸ì¼ìˆ˜ë¡ ë†’ì€ ê°’
   - ëª¨ë¸, ë°°ì¹˜, íŒŒë¼ë¯¸í„° ì„ íƒì— ë”°ë¼ ìŠ¤ì¼€ì¼ì´ í¬ê²Œ ë‹¬ë¼ì§
   - ì ˆëŒ€ì  ë¹„êµê°€ ì–´ë ¤ìš°ë¯€ë¡œ ë³´ì¡° ì§€í‘œë¡œ ì‚¬ìš© ê¶Œì¥

2. Cosine Similarity (ì½”ì‚¬ì¸ ìœ ì‚¬ë„): <g(x), g(x')> / (||g(x)|| * ||g(x')||)
   - ì •ê·œí™”ëœ ê·¸ë¼ë””ì–¸íŠ¸ ê°„ì˜ ê°ë„ ìœ ì‚¬ì„± (0~1)
   - í¬ê¸°ì— ë¬´ê´€í•˜ê²Œ ë°©í–¥ì„±ë§Œ ì¸¡ì •
   - ì„œë¡œ ë‹¤ë¥¸ ëª¨ë¸/ì„¤ì • ê°„ ë¹„êµê°€ ìš©ì´
   - í•´ì„ì´ ì§ê´€ì : 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ê°•í•œ ì •ë ¬, 0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì•½í•œ ì •ë ¬
   - ë©”ì¸ ì§€í‘œë¡œ ì‚¬ìš© ê¶Œì¥

USAGE RECOMMENDATION:
- ë³´ê³ ì„œë‚˜ ëª¨ë¸ ë¹„êµì—ëŠ” cosine similarityë¥¼ ì£¼ ì§€í‘œë¡œ ì‚¬ìš©
- inner productëŠ” gradient magnitude ë¶„ì„ ì‹œì—ë§Œ ì°¸ê³ 
- ì•”ê¸° ì „ì´ ê°•ë„ í‰ê°€: cosine > 0.7 (ê°•í•¨), 0.3~0.7 (ë³´í†µ), < 0.3 (ì•½í•¨)
"""

import torch
from torch import nn
import torch.nn.functional as F
from contextlib import nullcontext
from typing import List, Dict, Optional, Tuple
from dataclasses import dataclass
from transformers import AutoModelForCausalLM, AutoTokenizer, PreTrainedModel
import logging

logger = logging.getLogger(__name__)


def iter_trainable_params(model: PreTrainedModel, only_last_n_layers: Optional[int] = None) -> List[nn.Parameter]:
    """í•„ìš” ì‹œ ë§ˆì§€ë§‰ Nê°œ ë ˆì´ì–´ë§Œ ê·¸ë¼ë””ì–¸íŠ¸ ê³„ì‚°(ê°€ì†/ë©”ëª¨ë¦¬ ì ˆì•½)."""
    params = [p for p in model.parameters() if p.requires_grad]
    if only_last_n_layers is None:
        return params
    # ê°„ë‹¨í•˜ê²Œ ëì—ì„œ Nê°œì˜ íŒŒë¼ë¯¸í„° í…ì„œë¥¼ ì‚¬ìš© (ë ˆì´ì–´ ë‹¨ìœ„ë¡œ ë” ì •êµí™” ê°€ëŠ¥)
    return params[-only_last_n_layers:]


def _grads_to_cpu_fp32(params: List[nn.Parameter]) -> List[torch.Tensor]:
    """í˜„ì¬ paramsì˜ gradë¥¼ CPU FP32 í…ì„œ(í‰íƒ„) ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜."""
    cpu_grads: List[torch.Tensor] = []
    for p in params:
        if p.grad is None:
            cpu_grads.append(torch.empty(0, dtype=torch.float32))
            continue
        g_cpu = p.grad.detach().to(dtype=torch.float32, device="cpu").view(-1).contiguous()
        cpu_grads.append(g_cpu)
    return cpu_grads


def _cpu_grads_norm(cpu_grads: List[torch.Tensor]) -> float:
    norm_sq = 0.0
    for g in cpu_grads:
        if g.numel() == 0:
            continue
        g_fp32 = g.to(dtype=torch.float32)
        norm_sq += float(torch.dot(g_fp32, g_fp32))
    return float(norm_sq ** 0.5 + 1e-12)


def _dot_with_cpu_grads(params: List[nn.Parameter], cpu_grads_ref: List[torch.Tensor]) -> Tuple[float, float]:
    """í˜„ì¬ params.gradì™€ CPUì— ì €ì¥ëœ ê¸°ì¤€ grad ê°„ ë‚´ì ê³¼ í˜„ì¬ grad ë…¸ë¦„ì„ ê³„ì‚°.
    - ë‚´ì ê³¼ ë…¸ë¦„ì€ CPUì—ì„œ ê³„ì‚°í•´ GPU ë©”ëª¨ë¦¬ë¥¼ ìµœì†Œí™”í•œë‹¤.
    """
    dot_sum = 0.0
    norm_sq = 0.0
    for p, g_ref in zip(params, cpu_grads_ref):
        if p.grad is None or g_ref.numel() == 0:
            continue
        g_cpu = p.grad.detach().to(dtype=torch.float32, device="cpu").view(-1)
        g_ref_fp32 = g_ref.to(dtype=torch.float32)
        dot_sum += float(torch.dot(g_cpu, g_ref_fp32))
        norm_sq += float(torch.dot(g_cpu, g_cpu))
    return dot_sum, float(norm_sq ** 0.5 + 1e-12)


def sequence_loss(model: PreTrainedModel, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:
    """
    labelsëŠ” causal LM ê´€ë¡€ëŒ€ë¡œ input_idsë¥¼ í•œ ì¹¸ ì‹œí”„íŠ¸í•´ ê³„ì‚°.
    padding í† í°ì€ -100ìœ¼ë¡œ ë¬´ì‹œ.
    """
    labels = input_ids.clone()
    labels[attention_mask == 0] = -100
    out = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
    # HFëŠ” í‰ê·  í† í° CEë¥¼ ë°˜í™˜(ë¬´ì‹œ í† í° ì œì™¸) â†’ ì •ì˜ 3.1ì˜ ì‹œí€€ìŠ¤ í‰ê·  ì†ì‹¤ê³¼ í•©ì¹˜ê²Œ ì‚¬ìš©
    return out.loss


def sequence_backward(
    model: nn.Module,
    params: List[nn.Parameter],
    input_ids: torch.Tensor,
    attention_mask: torch.Tensor,
    fp16_autocast: bool = False,
    grad_scaler: Optional[torch.cuda.amp.GradScaler] = None,
) -> None:
    """
    ì‹œí€€ìŠ¤ì— ëŒ€í•œ backward pass ìˆ˜í–‰
    
    Args:
        grad_scaler: FP16 ì‚¬ìš© ì‹œ ìˆ˜ì¹˜ì  ì•ˆì •ì„±ì„ ìœ„í•œ gradient scaler.
                    ì¼ë°˜ì ìœ¼ë¡œ inference+backwardì—ì„œëŠ” í•„ìš”í•˜ì§€ ì•Šì§€ë§Œ,
                    ë§¤ìš° ì‘ì€ ëª¨ë¸ì´ë‚˜ ë°°ì¹˜ì—ì„œ underflow ìœ„í—˜ì´ ìˆì„ ìˆ˜ ìˆìŒ.
    """
    model.zero_grad(set_to_none=True)
    # NOTE: ê¸°ì¡´ full-backward ê²½ë¡œëŠ” ë¹„ê¶Œì¥(ì„±ëŠ¥ ë¹„íš¨ìœ¨). ìœ ì§€ë§Œ í•˜ê³  ì‚¬ìš©ì€ ì§€ì–‘.
    # ë””ë°”ì´ìŠ¤ì— ë”°ë¥¸ ì•ˆì „í•œ autocast ì²˜ë¦¬
    if input_ids.is_cuda:
        amp_ctx = torch.autocast("cuda", dtype=torch.float16, enabled=fp16_autocast)
    else:
        amp_ctx = nullcontext()
    with amp_ctx:
        loss = sequence_loss(model, input_ids, attention_mask)
    # ì¼ë°˜ì ì¸ backward (ë¶„ì„ ëª©ì ì—ì„  GradScaler ë¶ˆì‚¬ìš© ê¶Œì¥)
    loss.backward()
    
    # grads are now stored in params
    return None


@dataclass
class AlignOutputs:
    align_inner: float
    align_cosine: float
    grad_norm_orig: float
    grad_norm_para_mean: float


# ------------------------------
# Head-only gradient alignment
# ------------------------------

def _final_norm(model: PreTrainedModel, hidden: torch.Tensor) -> torch.Tensor:
    """ëª¨ë¸ ìœ í˜•ì— ë”°ë¼ ìµœì¢… LayerNormì„ ì ìš©(GPT: ln_f, LLaMA: model.norm)."""
    # LLaMA ê³„ì—´
    if hasattr(getattr(model, "model", None), "norm"):
        return model.model.norm(hidden)
    # GPT2/NeoX ê³„ì—´
    if hasattr(getattr(model, "transformer", None), "ln_f"):
        return model.transformer.ln_f(hidden)
    return hidden


def _encode(tokenizer: AutoTokenizer, texts, device: str, max_len: int):
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
        tokenizer.pad_token_id = tokenizer.eos_token_id
    enc = tokenizer(texts, return_tensors="pt", padding=True, truncation=True, max_length=max_len)
    return enc["input_ids"].to(device), enc["attention_mask"].to(device)


@torch.no_grad()
def _backbone_hidden(model: PreTrainedModel, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:
    """ë°±ë³¸ì€ no_gradë¡œë§Œ ì‚¬ìš©, ë§ˆì§€ë§‰ íˆë“ ì„ ì¶”ì¶œ í›„ í•„ìš” ì‹œ ìµœì¢… Norm ì ìš©."""
    out = model(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True, use_cache=False)
    hidden_last = out.hidden_states[-1]
    return _final_norm(model, hidden_last)


def _masked_ce(logits: torch.Tensor, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:
    """Causal LM ì‹œí”„íŠ¸ CEë¥¼ ë§ˆìŠ¤í¬ í‰ê· ìœ¼ë¡œ ê³„ì‚°(HFì˜ ìœ íš¨ í† í° í‰ê· ê³¼ ì¼ì¹˜)."""
    tgt = input_ids[:, 1:].contiguous()
    mask = attention_mask[:, 1:].contiguous()
    logits_use = logits[:, :-1, :].contiguous()
    v = logits_use.size(-1)
    loss_tok = F.cross_entropy(logits_use.view(-1, v), tgt.view(-1), reduction="none")
    loss_tok = loss_tok.view_as(tgt)
    denom = mask.sum().clamp_min(1)
    return (loss_tok * mask).sum() / denom


def _grad_head_vector(
    model: PreTrainedModel,
    hidden: torch.Tensor,
    input_ids: torch.Tensor,
    attention_mask: torch.Tensor,
    use_autocast: bool,
) -> Tuple[torch.Tensor, float]:
    """ì¶œë ¥ í—¤ë“œ íŒŒë¼ë¯¸í„°ì— ëŒ€í•´ì„œë§Œ ê·¸ë¼ë””ì–¸íŠ¸ ë²¡í„°ë¥¼ ê³„ì‚°í•˜ê³  CPU FP32ë¡œ ë°˜í™˜."""
    head = model.get_output_embeddings()
    if head is None:
        # fallback (ì¼ë¶€ ëª¨ë¸ì€ lm_headë§Œ ì¡´ì¬)
        head = getattr(model, "lm_head", None)
    if head is None:
        raise RuntimeError("Output head (lm_head) moduleì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    params = [p for p in head.parameters()]
    # ì•ˆì „í•œ autocast (CUDAì—ì„œë§Œ)
    amp_ctx = torch.autocast("cuda", dtype=torch.float16, enabled=use_autocast) if hidden.is_cuda else nullcontext()
    with amp_ctx:
        logits = head(hidden)  # [B, T, V]
        loss = _masked_ce(logits, input_ids, attention_mask)

    grads = torch.autograd.grad(loss, params, retain_graph=False, allow_unused=True)
    flat = []
    for g in grads:
        if g is None:
            continue
        flat.append(g.detach().to("cpu", dtype=torch.float32).view(-1))
    g_flat = torch.cat(flat, dim=0) if flat else torch.zeros(0, dtype=torch.float32)
    norm = float(torch.linalg.vector_norm(g_flat).item() + 1e-12)
    return g_flat, norm


def paraphrase_alignment_for_one(
    model: PreTrainedModel,
    tokenizer: AutoTokenizer,
    orig_text: str,
    paraphrases: List[str],
    params: Optional[List[nn.Parameter]] = None,  # deprecated in head-only path
    max_len: int = 512,
    device: str = "cuda",
    use_cosine: bool = True,
    fp16_autocast: bool = False,
) -> AlignOutputs:
    """í—¤ë“œ(ì¶œë ¥ì¸µ) íŒŒë¼ë¯¸í„°ì— ëŒ€í•´ì„œë§Œ ê·¸ë¼ë””ì–¸íŠ¸ ì •ë ¬ì„ ê³„ì‚°(íš¨ìœ¨ ëª¨ë“œ)."""
    # ì›ë³¸ íˆë“  (ë°±ë³¸ì€ no_grad)
    input_ids, attn = _encode(tokenizer, [orig_text], device, max_len)
    hidden = _backbone_hidden(model, input_ids, attn)
    g_ref, ref_norm = _grad_head_vector(model, hidden, input_ids, attn, fp16_autocast)

    inner_sum = 0.0
    cosine_sum = 0.0
    para_norm_sum = 0.0
    for p_text in paraphrases:
        input_ids_p, attn_p = _encode(tokenizer, [p_text], device, max_len)
        hidden_p = _backbone_hidden(model, input_ids_p, attn_p)
        g_p, p_norm = _grad_head_vector(model, hidden_p, input_ids_p, attn_p, fp16_autocast)

        # ë‚´ì /ì½”ì‚¬ì¸ ê³„ì‚° (CPU FP32)
        # ê¸¸ì´ ë¶ˆì¼ì¹˜ ê°€ëŠ¥ì„±ì€ ë‚®ì§€ë§Œ ì•ˆì „í•˜ê²Œ ê³µí†µ ê¸¸ì´ë¡œ ì²˜ë¦¬
        min_len = min(g_ref.numel(), g_p.numel())
        dot_val = float(torch.dot(g_ref[:min_len], g_p[:min_len]).item()) if min_len > 0 else 0.0
        inner_sum += dot_val
        para_norm_sum += p_norm
        if use_cosine:
            cosine_sum += dot_val / (ref_norm * p_norm + 1e-12)

    n = max(1, len(paraphrases))
    align_inner = inner_sum / n
    align_cosine = (cosine_sum / n) if use_cosine else float("nan")
    return AlignOutputs(
        align_inner=align_inner,
        align_cosine=align_cosine,
        grad_norm_orig=float(ref_norm),
        grad_norm_para_mean=float(para_norm_sum / n),
    )


class GradientAlignmentAnalyzer:
    """Gradient Alignment ë¶„ì„ê¸°"""
    
    def __init__(self, 
                 use_cosine: bool = True,
                 fp16_autocast: bool = False,
                 only_last_n_layers: Optional[int] = None,
                 max_len: int = 512):
        self.use_cosine = use_cosine
        self.fp16_autocast = fp16_autocast
        self.only_last_n_layers = only_last_n_layers
        self.max_len = max_len
        
    def analyze_batch(self,
                     model: nn.Module,
                     tokenizer: AutoTokenizer,
                     original_texts: List[str],
                     paraphrase_texts_list: List[List[str]],
                     device: str = "cuda") -> Dict:
        """
        ë°°ì¹˜ ì²˜ë¦¬ë¡œ ì—¬ëŸ¬ ì›ë¬¸ê³¼ íŒ¨ëŸ¬í”„ë ˆì´ì¦ˆì˜ gradient alignment ê³„ì‚°
        
        Args:
            model: ë¶„ì„í•  ëª¨ë¸
            tokenizer: í† í¬ë‚˜ì´ì €
            original_texts: ì›ë³¸ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            paraphrase_texts_list: ê° ì›ë³¸ì— ëŒ€í•œ íŒ¨ëŸ¬í”„ë ˆì´ì¦ˆ ë¦¬ìŠ¤íŠ¸ë“¤
            device: ë””ë°”ì´ìŠ¤
            
        Returns:
            ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        results = {
            'alignments': [],
            'align_inner_scores': [],
            'align_cosine_scores': [],
            'gradient_norms': {
                'original': [],
                'paraphrase': []
            }
        }
        
        # head-only ê²½ë¡œì—ì„œëŠ” params ì„œë¸Œì…‹ì´ í•„ìš” ì—†ìŒ
        model.eval()  # ì¼ê´€ì„±ì„ ìœ„í•´ eval ëª¨ë“œ
        
        logger.info(f"Computing gradient alignments for {len(original_texts)} samples...")
        
        for i, (orig_text, paraphrases) in enumerate(zip(original_texts, paraphrase_texts_list)):
            if not paraphrases:  # íŒ¨ëŸ¬í”„ë ˆì´ì¦ˆê°€ ì—†ìœ¼ë©´ ìŠ¤í‚µ
                continue
                
            try:
                # ê°œë³„ alignment ê³„ì‚°
                align_result = paraphrase_alignment_for_one(
                    model=model,
                    tokenizer=tokenizer,
                    orig_text=orig_text,
                    paraphrases=paraphrases,
                    max_len=self.max_len,
                    device=device,
                    use_cosine=self.use_cosine,
                    fp16_autocast=self.fp16_autocast
                )
                
                # ê²°ê³¼ ì €ì¥
                results['alignments'].append(align_result)
                results['align_inner_scores'].append(align_result.align_inner)
                results['align_cosine_scores'].append(align_result.align_cosine)
                
                # ê·¸ë¼ë””ì–¸íŠ¸ ë…¸ë¦„ ì €ì¥ (ìŠ¤ì¹¼ë¼ ê¸°ë°˜)
                results['gradient_norms']['original'].append(align_result.grad_norm_orig)
                results['gradient_norms']['paraphrase'].append(align_result.grad_norm_para_mean)
                
                if (i + 1) % 5 == 0:
                    logger.debug(f"Processed {i + 1}/{len(original_texts)} samples")
                    
            except Exception as e:
                logger.warning(f"Failed to process sample {i}: {e}")
                continue
        
        # í†µê³„ ê³„ì‚°
        results['statistics'] = self._compute_statistics(results)
        
        return results
    
    def _compute_statistics(self, results: Dict) -> Dict:
        """ê²°ê³¼ í†µê³„ ê³„ì‚°"""
        inner_scores = [x for x in results['align_inner_scores'] if not torch.isnan(torch.tensor(x))]
        cosine_scores = [x for x in results['align_cosine_scores'] if not torch.isnan(torch.tensor(x))]
        
        stats = {
            'num_samples': len(results['alignments']),
            'mean_align_inner': sum(inner_scores) / max(len(inner_scores), 1),
            'std_align_inner': torch.tensor(inner_scores).std().item() if len(inner_scores) > 1 else 0.0,
            'mean_align_cosine': sum(cosine_scores) / max(len(cosine_scores), 1),
            'std_align_cosine': torch.tensor(cosine_scores).std().item() if len(cosine_scores) > 1 else 0.0,
            'mean_grad_norm_orig': sum(results['gradient_norms']['original']) / max(len(results['gradient_norms']['original']), 1),
            'mean_grad_norm_para': sum(results['gradient_norms']['paraphrase']) / max(len(results['gradient_norms']['paraphrase']), 1)
        }
        
        return stats
    
    def compare_models(self,
                      model1: nn.Module,
                      model2: nn.Module, 
                      tokenizer: AutoTokenizer,
                      original_texts: List[str],
                      paraphrase_texts_list: List[List[str]],
                      device: str = "cuda",
                      model1_name: str = "Model1",
                      model2_name: str = "Model2") -> Dict:
        """ë‘ ëª¨ë¸ ê°„ì˜ gradient alignment ë¹„êµ"""
        
        logger.info(f"Comparing gradient alignments between {model1_name} and {model2_name}")
        
        # ê° ëª¨ë¸ì˜ alignment ê³„ì‚°
        results1 = self.analyze_batch(model1, tokenizer, original_texts, paraphrase_texts_list, device)
        results2 = self.analyze_batch(model2, tokenizer, original_texts, paraphrase_texts_list, device)
        
        # ë¹„êµ ê²°ê³¼
        comparison = {
            model1_name: results1,
            model2_name: results2,
            'comparison': {
                'inner_alignment_diff': results2['statistics']['mean_align_inner'] - results1['statistics']['mean_align_inner'],
                'cosine_alignment_diff': results2['statistics']['mean_align_cosine'] - results1['statistics']['mean_align_cosine'],
                'grad_norm_orig_diff': results2['statistics']['mean_grad_norm_orig'] - results1['statistics']['mean_grad_norm_orig'],
                'grad_norm_para_diff': results2['statistics']['mean_grad_norm_para'] - results1['statistics']['mean_grad_norm_para']
            }
        }
        
        return comparison


def create_alignment_report(analysis_results: Dict, title: str = "Gradient Alignment Analysis") -> str:
    """ë¶„ì„ ê²°ê³¼ ë¦¬í¬íŠ¸ ìƒì„±"""
    report = []
    report.append("=" * 60)
    report.append(title)
    report.append("=" * 60)
    
    if 'statistics' in analysis_results:
        stats = analysis_results['statistics']
        report.append(f"Samples processed: {stats['num_samples']}")
        report.append(f"Mean Inner Alignment: {stats['mean_align_inner']:.4f} (Â±{stats['std_align_inner']:.4f})")
        report.append(f"Mean Cosine Alignment: {stats['mean_align_cosine']:.4f} (Â±{stats['std_align_cosine']:.4f})")
        report.append(f"Mean Gradient Norm (Original): {stats['mean_grad_norm_orig']:.4f}")
        report.append(f"Mean Gradient Norm (Paraphrase): {stats['mean_grad_norm_para']:.4f}")
        report.append("")
        
        # í•´ì„
        cosine_align = stats['mean_align_cosine']
        if cosine_align > 0.7:
            report.append("ğŸ”¥ High gradient alignment - Strong memorization transfer detected")
        elif cosine_align > 0.3:
            report.append("âš ï¸  Moderate gradient alignment - Partial memorization transfer") 
        else:
            report.append("âœ… Low gradient alignment - Weak memorization transfer")
    
    report.append("=" * 60)
    return "\n".join(report)
