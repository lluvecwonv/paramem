# ğŸ“Š /root/memorization ì½”ë“œë² ì´ìŠ¤ ë¶„ì„

Generated: 2025-10-28

================================================================================
## ğŸ—ï¸ ì „ì²´ êµ¬ì¡° ê°œìš”
================================================================================

```
/root/memorization/
â””â”€â”€ mimir/                          # MIMIR íŒ¨í‚¤ì§€ (LLM Memorization ì¸¡ì • ë„êµ¬)
    â”œâ”€â”€ mimir/                      # Core MIMIR ë¼ì´ë¸ŒëŸ¬ë¦¬
    â”‚   â””â”€â”€ attacks/               # Membership Inference Attacks êµ¬í˜„
    â”œâ”€â”€ memorization/              # ğŸ¯ ì»¤ìŠ¤í…€ Memorization ë¶„ì„ ëª¨ë“ˆ (TOFU ì—°ë™)
    â”‚   â”œâ”€â”€ analysis/              # Paraphrase ë¶„ì„ ë„êµ¬
    â”‚   â”œâ”€â”€ config/                # ì„¤ì • íŒŒì¼ë“¤
    â”‚   â”œâ”€â”€ gradient/              # Gradient alignment ë¶„ì„
    â”‚   â”œâ”€â”€ prompts/               # Paraphrase ìƒì„± í”„ë¡¬í”„íŠ¸
    â”‚   â””â”€â”€ utils.py               # ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
    â”œâ”€â”€ data/                      # ë°ì´í„° íŒŒì¼
    â”œâ”€â”€ configs/                   # MIMIR ì‹¤í—˜ ì„¤ì •
    â”œâ”€â”€ scripts/                   # ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
    â”œâ”€â”€ analysis/                  # ê²°ê³¼ ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸
    â”œâ”€â”€ paraphrase_main.py         # ğŸ”¥ Paraphrase ë¶„ì„ ë©”ì¸ (TOFU ì—°ë™)
    â””â”€â”€ run.py                     # MIMIR MIA ì‹¤í—˜ ë©”ì¸
```

================================================================================
## ğŸ“¦ 1. MIMIR íŒ¨í‚¤ì§€ (Core Library)
================================================================================

### ëª©ì 
- LLMì˜ Memorizationì„ ì¸¡ì •í•˜ê¸° ìœ„í•œ íŒ¨í‚¤ì§€
- Membership Inference Attack (MIA) êµ¬í˜„
- ë…¼ë¬¸: "Do Membership Inference Attacks Work on Large Language Models?"

### ì£¼ìš” ì»´í¬ë„ŒíŠ¸

#### 1.1 `mimir/attacks/` - MIA ê³µê²© êµ¬í˜„
```
attacks/
â”œâ”€â”€ loss.py              # Likelihood-based attack
â”œâ”€â”€ ref.py               # Reference-based attack
â”œâ”€â”€ zlib.py              # Zlib entropy attack
â”œâ”€â”€ neighborhood.py      # Neighborhood attack
â”œâ”€â”€ min_k.py             # Min-K% Prob attack
â”œâ”€â”€ min_k_plus_plus.py   # Min-K%++ attack
â”œâ”€â”€ gradnorm.py          # Gradient Norm attack
â”œâ”€â”€ recall.py            # ReCaLL attack
â”œâ”€â”€ dc_pdd.py            # DC-PDD attack
â””â”€â”€ all_attacks.py       # Attack ì¸í„°í˜ì´ìŠ¤
```

**ê° ê³µê²© ë°©ë²• ì„¤ëª…:**

1. **Likelihood (loss)**: ë‹¨ìˆœíˆ íƒ€ê²Ÿ ë°ì´í„°ì˜ likelihoodë¥¼ ìŠ¤ì½”ì–´ë¡œ ì‚¬ìš©
2. **Reference-based (ref)**: Reference ëª¨ë¸ì˜ ìŠ¤ì½”ì–´ë¡œ ì •ê·œí™”
3. **Zlib Entropy**: Zlib ì••ì¶• í¬ê¸°ë¡œ ìƒ˜í”Œ ë‚œì´ë„ ì¶”ì •
4. **Neighborhood (ne)**: ë³´ì¡° ëª¨ë¸ë¡œ ì´ì›ƒ ìƒì„±, likelihood ë³€í™” ì¸¡ì •
5. **Min-K% Prob (min_k)**: ìµœì†Œ likelihoodë¥¼ ê°€ì§„ k% í† í° ì‚¬ìš©
6. **Min-K%++ (min_k++)**: ì •ê·œí™”ëœ likelihoodë¡œ Min-K% ê°œì„ 
7. **Gradient Norm (gradnorm)**: íƒ€ê²Ÿ ë°ì´í„°ì˜ gradient norm ì‚¬ìš©
8. **ReCaLL**: Unconditional vs conditional log-likelihood ë¹„êµ
9. **DC-PDD**: ëŒ€ê·œëª¨ ë§ë­‰ì¹˜ì˜ ë¹ˆë„ ë¶„í¬ë¡œ í† í° í™•ë¥  ë³´ì •

#### 1.2 `run.py` - MIMIR ì‹¤í—˜ ì‹¤í–‰
```python
# ì‚¬ìš©ë²•
python run.py --config configs/mi.json
```

- MIA ì‹¤í—˜ ì‹¤í–‰
- ë‹¤ì–‘í•œ ê³µê²© ë°©ë²• í…ŒìŠ¤íŠ¸
- ê²°ê³¼ë¥¼ `results/` ë””ë ‰í† ë¦¬ì— ì €ì¥

================================================================================
## ğŸ¯ 2. memorization/ ëª¨ë“ˆ (ì»¤ìŠ¤í…€ TOFU ì—°ë™)
================================================================================

### ëª©ì 
- TOFU unlearning ë°ì´í„°ì…‹ê³¼ ì—°ë™
- Paraphrase ê¸°ë°˜ memorization ì¸¡ì •
- Gradient alignment ë¶„ì„

### ë””ë ‰í† ë¦¬ êµ¬ì¡°

```
memorization/
â”œâ”€â”€ analysis/                    # Paraphrase ë¶„ì„ ë„êµ¬
â”‚   â”œâ”€â”€ paraphrase_analyzer.py  # Dual model ë¶„ì„ê¸°
â”‚   â”œâ”€â”€ paraphrase_generator.py # Paraphrase ìƒì„±ê¸°
â”‚   â”œâ”€â”€ paraphrase_visualizer.py# ì‹œê°í™” ë„êµ¬
â”‚   â””â”€â”€ paraphrase_generation_utils.py  # ìƒì„± ìœ í‹¸
â”‚
â”œâ”€â”€ config/                      # ì„¤ì • íŒŒì¼
â”‚   â”œâ”€â”€ model_config.yaml       # ëª¨ë¸ë³„ ì„¤ì • (llama2-7b, phi ë“±)
â”‚   â”œâ”€â”€ paraphrase_analysis.yaml# Paraphrase ë¶„ì„ ì„¤ì •
â”‚   â””â”€â”€ gradient_alignment.yaml # Gradient alignment ì„¤ì •
â”‚
â”œâ”€â”€ gradient/                    # Gradient ë¶„ì„
â”‚   â”œâ”€â”€ gradient_calculator.py  # Gradient ê³„ì‚°
â”‚   â””â”€â”€ alignment_analyzer.py   # Alignment ë¶„ì„
â”‚
â”œâ”€â”€ prompts/                     # Paraphrase í”„ë¡¬í”„íŠ¸
â”‚   â”œâ”€â”€ near_duplicate_1.txt    # 2ë‹¨ì–´ ë³€ê²½ ì œì•½
â”‚   â”œâ”€â”€ near_duplicate_2.txt
â”‚   â”œâ”€â”€ near_duplicate_3.txt
â”‚   â”œâ”€â”€ near_duplicate_4.txt
â”‚   â””â”€â”€ near_duplicate_5.txt
â”‚
â”œâ”€â”€ gradient_alignment_main.py  # Gradient alignment ì‹¤í—˜
â”œâ”€â”€ utils.py                    # ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
â””â”€â”€ notebook.ipynb              # ë¶„ì„ ë…¸íŠ¸ë¶
```

================================================================================
## ğŸ”¥ 3. paraphrase_main.py (TOFU ì—°ë™ ë©”ì¸)
================================================================================

### ì—­í• 
- TOFU ë°ì´í„°ì…‹ì—ì„œ paraphrase ê¸°ë°˜ memorization ì¸¡ì •
- Full model vs Retain model ë¹„êµ ë¶„ì„

### Workflow

```
Step 1: Load Models
  â”œâ”€â”€ Full model (trained on all data)
  â””â”€â”€ Retain model (trained without forget set)

Step 2: Analyze Original Questions
  â”œâ”€â”€ Generate answers with both models
  â”œâ”€â”€ Compute memorization scores (acc_in - acc_out)
  â””â”€â”€ Save to original_*.json

Step 3: Generate Paraphrases
  â”œâ”€â”€ Use ParaphraseGenerator
  â”œâ”€â”€ Apply generation mode (beam_1prompt, beam_5prompts, etc.)
  â””â”€â”€ Generate N paraphrases per question

Step 4: Analyze Paraphrased Questions
  â”œâ”€â”€ Generate answers with both models
  â”œâ”€â”€ Compute memorization scores
  â””â”€â”€ Save to paraphrase_*.json

Step 5: Compare & Visualize
  â”œâ”€â”€ Compare original vs paraphrase scores
  â”œâ”€â”€ Generate plots
  â””â”€â”€ Save results to results/{model}_{mode}_{split}/
```

### ì‹¤í–‰ ì˜ˆì‹œ

```bash
# TOFU í”„ë¡œì íŠ¸ì—ì„œ ì‹¤í–‰
cd /root/memorization_unlearn/TOFU
./run_gradient_alignment.sh
```

================================================================================
## ğŸ“Š 4. ì£¼ìš” í´ë˜ìŠ¤ ë° í•¨ìˆ˜
================================================================================

### 4.1 ParaphraseGenerator (analysis/paraphrase_generator.py)

**ëª©ì **: ë‹¤ì–‘í•œ ì „ëµìœ¼ë¡œ paraphrase ìƒì„±

**Generation Modes**:
1. `greedy_5prompts`: Greedy decoding + 5 diverse prompts
2. `beam_1prompt`: Beam search + 1 prompt
3. `beam_5prompts`: Beam search + 5 prompts
4. `nucleus_5prompts`: Nucleus sampling + 5 prompts

**ì£¼ìš” ë©”ì„œë“œ**:
```python
generate_qa_paraphrases(combined_text, model, tokenizer)
  â†’ List[str]  # Paraphrased QA texts
```

### 4.2 DualModelAnalyzer (analysis/paraphrase_analyzer.py)

**ëª©ì **: Full modelê³¼ Retain modelì„ ë¹„êµí•˜ì—¬ memorization ì¸¡ì •

**ì£¼ìš” ë©”ì„œë“œ**:
```python
run_dual_model_analysis(full_model, retain_model, tokenizer, dataset, tag)
  â†’ List[Dict]  # Results with memorization scores
```

**ê³„ì‚° ê³µì‹**:
```python
memorization = acc_in - acc_out
simplicity = acc_in + acc_out

where:
  acc_in = Full model accuracy (trained on all data)
  acc_out = Retain model accuracy (trained without forget set)
```

### 4.3 ParaphraseVisualizer (analysis/paraphrase_visualizer.py)

**ëª©ì **: ê²°ê³¼ ì‹œê°í™”

**ìƒì„± í”Œë¡¯**:
- Memorization vs Simplicity scatter plot
- Original vs Paraphrase comparison
- Distribution histograms

### 4.4 FullQADataset (utils.py)

**ëª©ì **: TOFU ë°ì´í„°ì…‹ì„ PyTorch Datasetìœ¼ë¡œ ë˜í•‘

**í˜•ì‹**:
```python
# Input format
"[INST] {question} [/INST] {answer}"  # llama2-7b
"Question: {question}\nAnswer: {answer}"  # phi

# Output
(input_ids, labels, attention_mask)
```

================================================================================
## âš™ï¸ 5. ì„¤ì • íŒŒì¼ë“¤
================================================================================

### 5.1 model_config.yaml

ëª¨ë¸ë³„ í˜•ì‹ ì •ì˜:
```yaml
llama2-7b:
  question_start_tag: "[INST] "
  question_end_tag: " [/INST]"
  answer_tag: ""

phi:
  question_start_tag: "Question: "
  question_end_tag: "\n"
  answer_tag: "Answer: "
```

### 5.2 paraphrase_analysis.yaml

Paraphrase ë¶„ì„ ì„¤ì •:
```yaml
model_family: llama2-7b
split: forget10  # forget10, forget05, forget01

analysis:
  generation_mode: beam_1prompt
  num_paraphrases: 5
  num_beams: 4

generation:
  max_length: 256
  max_new_tokens: 256
```

### 5.3 gradient_alignment.yaml

Gradient alignment ë¶„ì„ ì„¤ì •:
```yaml
model_family: llama2-7b
data_path: locuslab/TOFU
split: forget10

alignment:
  batch_size: 1
  num_samples: 100
```

================================================================================
## ğŸ“ 6. ë°ì´í„° íë¦„
================================================================================

### Input Data (TOFU Dataset)
```
locuslab/TOFU
â”œâ”€â”€ forget10/  # 10% forget set
â”œâ”€â”€ forget05/  # 5% forget set
â”œâ”€â”€ forget01/  # 1% forget set
â””â”€â”€ retain/    # Retain set
```

### Output Results
```
results/
â”œâ”€â”€ {model}_{mode}_{split}/
â”‚   â”œâ”€â”€ original_locuslab_TOFU_{split}_results.json
â”‚   â”œâ”€â”€ paraphrase_locuslab_TOFU_{split}_results.json
â”‚   â”œâ”€â”€ memorization_simplicity_original.png
â”‚   â”œâ”€â”€ memorization_simplicity_paraphrase.png
â”‚   â””â”€â”€ DECODING_STRATEGY.txt
```

### JSON ê²°ê³¼ í˜•ì‹
```json
{
  "results": [
    {
      "Question": "[INST] ... [/INST]",
      "GroundTruth": "...",
      "Predicted": "...",
      "OriginalMemorization": 0.41,
      "OriginalSimplicity": 1.58,
      "ParaphraseResults": [
        {
          "paraphrased_question": "...",
          "paraphrase_memorization": 0.35,
          "memorization_difference": -0.06,
          ...
        }
      ]
    }
  ]
}
```

================================================================================
## ğŸ”¬ 7. ì‹¤í—˜ íŒŒì´í”„ë¼ì¸
================================================================================

### 7.1 MIMIR MIA ì‹¤í—˜
```bash
# 1. Configure
vim configs/mi.json

# 2. Run
python run.py --config configs/mi.json

# 3. Results
results/
â””â”€â”€ {experiment_name}/
    â”œâ”€â”€ scores.json
    â””â”€â”€ metrics.json
```

### 7.2 TOFU Paraphrase ì‹¤í—˜
```bash
# 1. Configure
vim memorization/config/paraphrase_analysis.yaml

# 2. Run (from TOFU project)
cd /root/memorization_unlearn/TOFU
./run_gradient_alignment.sh

# 3. Results
results/{model}_{mode}_{split}/
â”œâ”€â”€ original_*.json
â”œâ”€â”€ paraphrase_*.json
â””â”€â”€ plots/
```

================================================================================
## ğŸ“ 8. í•µì‹¬ ì»¨ì…‰
================================================================================

### 8.1 Memorization Score
```
Memorization = Accuracy_in - Accuracy_out

where:
- Accuracy_in: Full model (trained on all data)
- Accuracy_out: Retain model (trained without forget set)

High memorization â†’ Model memorized the forget set
Low memorization â†’ Model generalized (no memorization)
```

### 8.2 Paraphrase-based Testing
```
Original Question â†’ [Paraphrase] â†’ Paraphrased Question
                                          â†“
                                   Does model still remember?
                                          â†“
                    If yes â†’ True memorization (not just keyword matching)
                    If no â†’ Shallow memorization (overfitting to wording)
```

### 8.3 Decoding Strategies
```
1. Greedy: Deterministic, fast, low diversity
2. Beam: Deterministic, medium speed, medium diversity
3. Nucleus: Stochastic, slower, high diversity

Current setup:
  - beam_1prompt with 4 beams
  - 2-word change constraint
  - min_new_tokens=10, repetition_penalty=1.1
```

================================================================================
## ğŸ”— 9. TOFU í”„ë¡œì íŠ¸ì™€ì˜ ì—°ë™
================================================================================

### 9.1 í†µí•© êµ¬ì¡°
```
/root/memorization_unlearn/TOFU/  (ë©”ì¸ í”„ë¡œì íŠ¸)
â”œâ”€â”€ memorization/                  (ì‹¬ë³¼ë¦­ ë§í¬ ë˜ëŠ” ë³µì‚¬)
â”‚   â†’ /root/memorization/mimir/memorization/
â”‚
â”œâ”€â”€ run_gradient_alignment.sh     (ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸)
â”‚   â†’ torchrun memorization/gradient_alignment_main.py
â”‚
â””â”€â”€ results/                      (ê²°ê³¼ ì €ì¥)
    â””â”€â”€ {model}_{mode}_{split}/
```

### 9.2 Import ê²½ë¡œ
```python
# TOFU í”„ë¡œì íŠ¸ì—ì„œ
from memorization.analysis.paraphrase_analyzer import DualModelAnalyzer
from memorization.analysis.paraphrase_generator import ParaphraseGenerator
from memorization.utils import FullQADataset
```

================================================================================
## ğŸ“ 10. ì£¼ìš” íŒŒì¼ ìš”ì•½
================================================================================

| íŒŒì¼ | ì—­í•  | ì¤‘ìš”ë„ |
|------|------|--------|
| `paraphrase_main.py` | TOFU ì—°ë™ ë©”ì¸ ìŠ¤í¬ë¦½íŠ¸ | â­â­â­â­â­ |
| `memorization/analysis/paraphrase_generator.py` | Paraphrase ìƒì„± | â­â­â­â­â­ |
| `memorization/analysis/paraphrase_analyzer.py` | Dual model ë¶„ì„ | â­â­â­â­â­ |
| `memorization/analysis/paraphrase_generation_utils.py` | ìƒì„± ìœ í‹¸ | â­â­â­â­ |
| `memorization/utils.py` | Dataset, ì €ì¥ í•¨ìˆ˜ | â­â­â­â­ |
| `memorization/config/paraphrase_analysis.yaml` | ì‹¤í—˜ ì„¤ì • | â­â­â­â­ |
| `memorization/prompts/near_duplicate_*.txt` | Paraphrase í”„ë¡¬í”„íŠ¸ | â­â­â­ |
| `mimir/attacks/*.py` | MIA ê³µê²© êµ¬í˜„ | â­â­â­ |
| `run.py` | MIMIR MIA ì‹¤í—˜ | â­â­â­ |

================================================================================
## ğŸš€ 11. ë‹¤ìŒ ë‹¨ê³„
================================================================================

### í˜„ì¬ ìƒíƒœ
âœ… Paraphrase generator ìˆ˜ì • ì™„ë£Œ (num_beams=4, min_new_tokens=10)
âœ… 2-word change constraint í™•ì¸
âœ… Split ì •ë³´ í¬í•¨í•œ ê²°ê³¼ ì €ì¥ ê²½ë¡œ ì„¤ì •
âœ… Documentation ì‘ì„±

### ì‹¤í–‰ ëŒ€ê¸°
â³ ìƒˆ decoding strategyë¡œ ì‹¤í—˜ ì‹¤í–‰
â³ ê²°ê³¼ ë¶„ì„ ë° ë¹„êµ
â³ Memorization score ë³€í™” í™•ì¸

================================================================================
END OF CODEBASE ANALYSIS
================================================================================
